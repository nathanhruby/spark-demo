{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuson Python Meetup Spark Demo\n",
    "## Hello and welcome!\n",
    "\n",
    "In this demo we will use this Jupyter Notebook to write a small program to parse an apache access log and then query the results.  This workshop is a hand-on class from teh previous demo, using the same data.  Spark is a large project with a huge surface area.  This demo will only focus on the SQL-like capabilities of Spark and offer code samples.\n",
    "\n",
    "## You will need...\n",
    "- Docker CE\n",
    "  - Windows and Mac folks should download and install Docker CE using the provider installers from [Docker](https://store.docker.com/search?type=edition&offering=community)\n",
    "  - Linux folks should install Docker using their distribution provided packages\n",
    "- Git\n",
    "  - Windows folks can download git [here](https://git-scm.com/downloads)\n",
    "  - Mac folks should should have CLI git clients aleady \n",
    "  - Linux folks may need need install a seperate package\n",
    " \n",
    "## Please raise your hand...\n",
    "- if you have a question.\n",
    "- if you are having difficulty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space... The Final Frontier...\n",
    "#\n",
    "# ... \n",
    "#\n",
    "# These are the voyages of the the startship \"don't read ahead\" ....\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step One\n",
    "We need to import Spark and create a session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# This creates a spark context to operate in: use the local machine and all avalaible processors\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "\n",
    "# This create a session in which we can build tables and\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Two\n",
    "We need to parse a log data line which looks like this\n",
    "```\n",
    "109.169.248.247 - - [12/Dec/2015:18:25:11 +0100] \"GET /administrator/ HTTP/1.1\" 200 4263 \"-\" \"Mozilla/5.0 (Windows NT 6.0; rv:34.0) Gecko/20100101 Firefox/34.0\" \"-\"\n",
    "```\n",
    "Into something meaningful like this:\n",
    "\n",
    "| key | ip addr | identd | http auth | Datetime | Request (verb path proto) | Response | Bytes Sent | Referer | User-Agent | ? |\n",
    "|-----|---------|---|---|----------|-----------------------------|---------------|------------|---|------------|---|\n",
    "|line | `109.169.248.247` | `-` | `-` | `[12/Dec/2015:18:25:11 +0100]` | `\"GET /administrator/ HTTP/1.1\"` | `200` | `4263` | `\"-\"` | `\"Mozilla/5.0 (Windows NT 6.0; rv:34.0) Gecko/20100101 Firefox/34.0\"` | `\"-\"` |\n",
    "|regex|`^(\\S+)` | `(\\S+)` | `(\\S+)` | `\\[([\\w:/]+\\s[+\\-]\\d{4})\\]` | `\"(\\S+) (\\S+) (\\S+)\"` | `(\\d{3})` | `(\\d+)?` | `\"(.*)\"` | `\"(.*)\"` | `\"-\"` |\n",
    "\n",
    "You will note that we don't know what that last field is.  It's often vhost, but in our dataset it's only `\"-\"` so we will assume it is some site specific field that is scrubbed and not of our concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "APACHE_ACCESS_LOG_PATTERN = '^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+) (\\S+)\" (\\d{3}) (\\d+|-) \"(.*)\" \"(.*)\" \"-\"'\n",
    "\n",
    "\n",
    "# Returns a dictionary containing the parts of the Apache Access Log.\n",
    "def parse_apache_log_line(logline):\n",
    "    match = re.search(APACHE_ACCESS_LOG_PATTERN, logline)\n",
    "    if match is None:\n",
    "        # Optionally, you can change this to just ignore if each line of data is not critical.\n",
    "        # For this example, we want to ensure that the format is consistent.\n",
    "        raise Exception(\"Invalid logline: %s\" % logline)\n",
    "    # This is a quirk of apache, if there is no value, it will often log a \"-\" instead to \n",
    "    # indicate a null value.  For size, this causes typing problems for our field, so \n",
    "    # fixup this field with a little processing logic\n",
    "    if match.group(9) in (None, \"-\"):\n",
    "        size = 0\n",
    "    else:\n",
    "        size = match.group(9)\n",
    "    return Row(\n",
    "        ipAddress    = str(match.group(1)),\n",
    "        clientIdentd = str(match.group(2)),\n",
    "        userId       = str(match.group(3)),\n",
    "        dateTime     = str(match.group(4)),\n",
    "        verb         = str(match.group(5)),\n",
    "        path         = str(match.group(6)),\n",
    "        protocol     = str(match.group(7)),\n",
    "        responseCode = int(match.group(8)),\n",
    "        contentSize  = int(size),\n",
    "        referer      = str(match.group(10)),\n",
    "        user_agent   = str(match.group(11))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Three\n",
    "We need to load our log files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_files = \"file:///home/jovyan/work/access.log\"\n",
    "raw_log_files = sc.textFile(log_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Three and a Half\n",
    "Once loaded, we need to parse them and make the result a SQL table for us to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_log_files = raw_log_files.map(parse_apache_log_line)\n",
    "parsed_log_files.toDF().registerTempTable(\"log_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Four\n",
    "We need to query our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF = spark.sql(\"SELECT count(*) AS count, ipAddress FROM log_data GROUP BY ipAddress ORDER BY count DESC\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we wanted to order by date?\n",
    "Spark has a User Defined Function (UDF) for that.  UDF's are in java [SimpleDateFormat](https://docs.oracle.com/javase/6/docs/api/java/text/SimpleDateFormat.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT ipAddress, dateTime, to_timestamp(dateTime, 'dd/MMM/yyyy:HH:mm:ss Z') AS date FROM log_data ORDER BY date DESC LIMIT 10\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we wanted to run our data though an external program?\n",
    "Let's assume we needed to process each log line in the RDD  with a custom program and then work on the resulting set.  Spark offers the pipe() transformation to process RDD's though an external program.  Pipe() expects the program to act as a FIFO with newline terminated lines going into stdin and new data emitted on stdout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_md5_sums = raw_log_files.pipe(\"/usr/bin/rev\")\n",
    "log_md5_sums.collect()\n",
    "# my external\n",
    "print(log_md5_sums.take(1))\n",
    "# mostly the same but with python (casting the result as sting stringifies the [] hence the weirdness at ends)\n",
    "print(str(raw_log_files.take(1))[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
